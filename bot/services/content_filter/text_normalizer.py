# ============================================================
# TEXT NORMALIZER - НОРМАЛИЗАЦИЯ ТЕКСТА
# ============================================================
# Этот модуль отвечает за нормализацию текста перед проверкой:
# - Замена похожих символов (l33tspeak): 0→о, 1→и, @→а
# - Удаление разделителей между буквами: к-о-к-а → кока
# - Удаление невидимых символов (zero-width)
# - Приведение к нижнему регистру
#
# Это позволяет ловить обфускацию типа: wишki, ko—k-a, eKct-zy
# ============================================================

# Импортируем модуль регулярных выражений для паттернов
import re
# Импортируем тип для аннотаций
from typing import Optional


class TextNormalizer:
    """
    Класс для нормализации текста перед проверкой на запрещённые слова.

    Решает проблему обхода фильтров через:
    - Замену букв на похожие символы (0 вместо о)
    - Вставку разделителей (к-о-к-а-и-н)
    - Использование невидимых символов

    Пример использования:
        normalizer = TextNormalizer()
        normalized = normalizer.normalize("wишki ko—k-a")
        # Результат: "вишки кока"
    """

    # ─────────────────────────────────────────────────────────
    # СЛОВАРЬ ЗАМЕНЫ ПОХОЖИХ СИМВОЛОВ
    # ─────────────────────────────────────────────────────────
    # Ключ = символ который нужно заменить
    # Значение = на что заменяем (кириллица)
    # Включает латиницу, цифры и спецсимволы похожие на буквы
    CHAR_MAP = {
        # Латинские буквы → кириллица (визуально похожие)
        'a': 'а',  # латинская a → кириллическая а
        'b': 'б',  # латинская b → кириллическая б (приблизительно)
        'c': 'с',  # латинская c → кириллическая с
        'd': 'д',  # латинская d → кириллическая д (приблизительно)
        'e': 'е',  # латинская e → кириллическая е
        'f': 'ф',  # латинская f → кириллическая ф (приблизительно)
        'g': 'г',  # латинская g → кириллическая г (приблизительно)
        'h': 'н',  # латинская h → кириллическая н (приблизительно)
        'i': 'и',  # латинская i → кириллическая и
        'k': 'к',  # латинская k → кириллическая к
        'l': 'л',  # латинская l → кириллическая л (приблизительно)
        'm': 'м',  # латинская m → кириллическая м
        'n': 'н',  # латинская n → кириллическая н
        'o': 'о',  # латинская o → кириллическая о
        'p': 'р',  # латинская p → кириллическая р
        'r': 'р',  # латинская r → кириллическая р (приблизительно)
        's': 'с',  # латинская s → кириллическая с (приблизительно)
        't': 'т',  # латинская t → кириллическая т
        'u': 'у',  # латинская u → кириллическая у
        'v': 'в',  # латинская v → кириллическая в (приблизительно)
        'w': 'в',  # латинская w → кириллическая в (часто используется)
        'x': 'х',  # латинская x → кириллическая х
        'y': 'у',  # латинская y → кириллическая у (приблизительно)
        'z': 'з',  # латинская z → кириллическая з (приблизительно)

        # Цифры → буквы (l33tspeak)
        '0': 'о',  # ноль похож на букву о
        '1': 'и',  # единица похожа на и (или l→л)
        '3': 'з',  # тройка похожа на з (или е в английском)
        '4': 'ч',  # четвёрка похожа на ч (или a в английском)
        '5': 'с',  # пятёрка похожа на s→с
        '6': 'б',  # шестёрка похожа на б (приблизительно)
        '7': 'т',  # семёрка похожа на т (приблизительно)
        '8': 'в',  # восьмёрка похожа на в (или b)
        '9': 'д',  # девятка похожа на д (приблизительно)

        # Спецсимволы → буквы
        '@': 'а',  # собака похожа на а
        '$': 'с',  # доллар похож на s→с
        '€': 'е',  # евро похож на е
        '₽': 'р',  # рубль похож на р
        '!': 'и',  # восклицательный знак иногда заменяет i→и
        '|': 'и',  # вертикальная черта похожа на и или l

        # Заглавные латинские (дополнительно к lower())
        'A': 'а',
        'B': 'б',
        'C': 'с',
        'E': 'е',
        'H': 'н',
        'K': 'к',
        'M': 'м',
        'O': 'о',
        'P': 'р',
        'T': 'т',
        'X': 'х',
    }

    # ─────────────────────────────────────────────────────────
    # СИМВОЛЫ-РАЗДЕЛИТЕЛИ (удаляем)
    # ─────────────────────────────────────────────────────────
    # Эти символы часто вставляют между буквами для обхода фильтра
    # Пример: к-о-к-а-и-н, э.к" с.т" а.з" и
    SEPARATORS = [
        '-',   # дефис: к-о-к-а
        '–',   # короткое тире
        '—',   # длинное тире
        '_',   # нижнее подчёркивание: ко_ка
        '.',   # точка: к.о.к.а
        ',',   # запятая
        ' ',   # пробел: к о к а (удаляем только между буквами, не всегда)
        '*',   # звёздочка: к*о*к*а
        "'",   # апостроф
        '"',   # кавычки
        '`',   # обратный апостроф
        '~',   # тильда
        '+',   # плюс
        '=',   # равно
        ':',   # двоеточие
        ';',   # точка с запятой
        '/',   # слэш
        '\\',  # обратный слэш
        '|',   # вертикальная черта
    ]

    # ─────────────────────────────────────────────────────────
    # НЕВИДИМЫЕ СИМВОЛЫ (zero-width)
    # ─────────────────────────────────────────────────────────
    # Эти символы не отображаются, но разбивают слово
    # Используются для обхода фильтров
    INVISIBLE_CHARS = [
        '\u200b',  # Zero Width Space
        '\u200c',  # Zero Width Non-Joiner
        '\u200d',  # Zero Width Joiner
        '\u200e',  # Left-to-Right Mark
        '\u200f',  # Right-to-Left Mark
        '\u2060',  # Word Joiner
        '\u2061',  # Function Application
        '\u2062',  # Invisible Times
        '\u2063',  # Invisible Separator
        '\u2064',  # Invisible Plus
        '\ufeff',  # Byte Order Mark (BOM)
        '\u00ad',  # Soft Hyphen
        '\u034f',  # Combining Grapheme Joiner
        '\u061c',  # Arabic Letter Mark
        '\u115f',  # Hangul Choseong Filler
        '\u1160',  # Hangul Jungseong Filler
        '\u17b4',  # Khmer Vowel Inherent Aq
        '\u17b5',  # Khmer Vowel Inherent Aa
        '\u180e',  # Mongolian Vowel Separator
        '\u2000',  # En Quad (различные виды пробелов)
        '\u2001',  # Em Quad
        '\u2002',  # En Space
        '\u2003',  # Em Space
        '\u2004',  # Three-Per-Em Space
        '\u2005',  # Four-Per-Em Space
        '\u2006',  # Six-Per-Em Space
        '\u2007',  # Figure Space
        '\u2008',  # Punctuation Space
        '\u2009',  # Thin Space
        '\u200a',  # Hair Space
        '\u202f',  # Narrow No-Break Space
        '\u205f',  # Medium Mathematical Space
        '\u3000',  # Ideographic Space
    ]

    def __init__(self):
        """
        Инициализация нормализатора.

        Компилируем регулярные выражения один раз при создании объекта
        для повышения производительности при многократном использовании.
        """
        # Компилируем паттерн для удаления невидимых символов
        # Создаём класс символов [...] из списка
        invisible_pattern = '[' + ''.join(re.escape(c) for c in self.INVISIBLE_CHARS) + ']'
        # Компилируем regex для многократного использования
        self._invisible_regex = re.compile(invisible_pattern)

        # Компилируем паттерн для удаления разделителей между буквами
        # Ищем: буква + разделитель + буква, заменяем на: буква + буква
        separator_chars = ''.join(re.escape(c) for c in self.SEPARATORS)
        # Паттерн: (буква)(один или более разделителей)(буква)
        # \w не подходит идеально, используем Unicode letter categories
        self._separator_regex = re.compile(
            r'(\w)[' + separator_chars + r']+(\w)',
            re.UNICODE
        )

    def normalize(self, text: str) -> str:
        """
        Нормализует текст для сравнения с запрещёнными словами.

        Этапы нормализации:
        1. Приведение к нижнему регистру
        2. Удаление невидимых символов
        3. Удаление разделителей между буквами
        4. Замена похожих символов на кириллицу

        Args:
            text: Исходный текст для нормализации

        Returns:
            Нормализованный текст (нижний регистр, без обфускации)

        Example:
            >>> normalizer = TextNormalizer()
            >>> normalizer.normalize("wИшKi")
            'вишки'
            >>> normalizer.normalize("ko—k-a")
            'кока'
            >>> normalizer.normalize("L s_D")
            'лсд'
        """
        # Проверяем на пустой или None текст
        if not text:
            # Возвращаем пустую строку если входные данные пустые
            return ''

        # ─────────────────────────────────────────────────────────
        # ШАГ 1: Приводим к нижнему регистру
        # ─────────────────────────────────────────────────────────
        # Это упрощает дальнейшее сравнение
        # "КОКАИН" и "кокаин" становятся одинаковыми
        result = text.lower()

        # ─────────────────────────────────────────────────────────
        # ШАГ 2: Удаляем невидимые символы
        # ─────────────────────────────────────────────────────────
        # Zero-width символы могут разбивать слово:
        # "ко\u200bкаин" выглядит как "кокаин", но не матчится
        result = self._invisible_regex.sub('', result)

        # ─────────────────────────────────────────────────────────
        # ШАГ 3: Удаляем разделители между буквами
        # ─────────────────────────────────────────────────────────
        # "к-о-к-а" → "кока"
        # Делаем несколько проходов, т.к. regex убирает по одному
        # Пример: "к-о-к" → "ко-к" → "кок"
        prev_result = None
        # Повторяем пока есть изменения (максимум 10 итераций для защиты)
        iterations = 0
        while prev_result != result and iterations < 10:
            # Сохраняем предыдущий результат для сравнения
            prev_result = result
            # Заменяем: буква + разделители + буква → буква + буква
            result = self._separator_regex.sub(r'\1\2', result)
            # Увеличиваем счётчик итераций
            iterations += 1

        # ─────────────────────────────────────────────────────────
        # ШАГ 4: Заменяем похожие символы на кириллицу
        # ─────────────────────────────────────────────────────────
        # "wишki" → "вишки"
        # "k0ka" → "кока"
        # Проходим по каждому символу и заменяем если есть в словаре
        normalized_chars = []
        for char in result:
            # Если символ есть в словаре замен - заменяем
            if char in self.CHAR_MAP:
                normalized_chars.append(self.CHAR_MAP[char])
            else:
                # Иначе оставляем как есть
                normalized_chars.append(char)

        # Собираем обратно в строку
        result = ''.join(normalized_chars)

        # Возвращаем нормализованный текст
        return result

    def normalize_word(self, word: str) -> str:
        """
        Нормализует отдельное слово (без пробелов).

        Отличие от normalize(): убирает ВСЕ пробелы,
        а не только между буквами.

        Args:
            word: Слово для нормализации

        Returns:
            Нормализованное слово без пробелов
        """
        # Убираем все пробелы из слова
        word_no_spaces = word.replace(' ', '')
        # Применяем стандартную нормализацию
        return self.normalize(word_no_spaces)

    def get_words_from_text(self, text: str) -> list[str]:
        """
        Извлекает отдельные слова из текста после нормализации.

        Полезно для проверки каждого слова по отдельности.

        Args:
            text: Исходный текст

        Returns:
            Список нормализованных слов

        Example:
            >>> normalizer = TextNormalizer()
            >>> normalizer.get_words_from_text("Привет wишki!")
            ['привет', 'вишки']
        """
        # Сначала нормализуем весь текст
        normalized = self.normalize(text)
        # Разбиваем по не-буквенным символам
        # \W+ означает один или более не-словесных символов
        words = re.split(r'\W+', normalized, flags=re.UNICODE)
        # Фильтруем пустые строки
        return [w for w in words if w]


# ============================================================
# ГЛОБАЛЬНЫЙ ЭКЗЕМПЛЯР ДЛЯ ПЕРЕИСПОЛЬЗОВАНИЯ
# ============================================================
# Создаём один экземпляр нормализатора для всего приложения
# Это экономит память и время на компиляцию regex
_normalizer_instance: Optional[TextNormalizer] = None


def get_normalizer() -> TextNormalizer:
    """
    Возвращает глобальный экземпляр TextNormalizer.

    Использует паттерн Singleton для переиспользования.

    Returns:
        TextNormalizer: Глобальный экземпляр нормализатора
    """
    global _normalizer_instance
    # Создаём экземпляр только при первом вызове
    if _normalizer_instance is None:
        _normalizer_instance = TextNormalizer()
    return _normalizer_instance
